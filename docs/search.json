[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Suvaditya Mukherjee",
    "section": "",
    "text": "Accelerating Model Deployment using Transfer Learning and Vertex AI\n\n\n\n\n\n\n\nmlops\n\n\ndocker\n\n\ncomputer-vision\n\n\nai\n\n\n\n\nGetting started with using Docker and TF Serving on GCP\n\n\n\n\n\n\nJan 3, 2023\n\n\nSuvaditya Mukherjee\n\n\n\n\n\n\n  \n\n\n\n\nHow to make your models available to the public\n\n\n\n\n\n\n\nmlops\n\n\ndocker\n\n\ncomputer-vision\n\n\nai\n\n\n\n\nExplaining how to use Docker, Flask and Gunicorn to deploy your models online\n\n\n\n\n\n\nSep 5, 2022\n\n\nSuvaditya Mukherjee\n\n\n\n\n\n\n  \n\n\n\n\nThe Annotated ResNet-50\n\n\n\n\n\n\n\nscratch-models\n\n\ncomputer-vision\n\n\nai\n\n\n\n\nExplaining how ResNet-50 works and why it is so popular\n\n\n\n\n\n\nAug 5, 2022\n\n\nSuvaditya Mukherjee\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-08-05-the-annotated-resnet.html",
    "href": "posts/2022-08-05-the-annotated-resnet.html",
    "title": "The Annotated ResNet-50",
    "section": "",
    "text": "The ResNet architecture is considered to be among the most popular Convolutional Neural Network architectures around. Introduced by Microsoft Research in 2015, Residual Networks (ResNet in short) broke several records when it was first introduced in this paper by He. et. al"
  },
  {
    "objectID": "posts/2022-08-05-the-annotated-resnet.html#introduction",
    "href": "posts/2022-08-05-the-annotated-resnet.html#introduction",
    "title": "The Annotated ResNet-50",
    "section": "",
    "text": "The ResNet architecture is considered to be among the most popular Convolutional Neural Network architectures around. Introduced by Microsoft Research in 2015, Residual Networks (ResNet in short) broke several records when it was first introduced in this paper by He. et. al"
  },
  {
    "objectID": "posts/2022-08-05-the-annotated-resnet.html#why-resnet",
    "href": "posts/2022-08-05-the-annotated-resnet.html#why-resnet",
    "title": "The Annotated ResNet-50",
    "section": "Why ResNet?",
    "text": "Why ResNet?\nThe requirement for a model like ResNet arose due to a number of pitfalls in modern networks at the time.\n\nDifficulty in training deep neural networks: As the number of layers in a model increases, the number of parameters in the model increases exponentially. For each Convolutional layer, a total of \\(((height_{kernel} \\cdot width_{kernel} \\cdot filters_{input}) + 1) \\cdot filters_{output}\\) gets added to the bill. To put it into context, a simple 7x7 kernel Convolution layer from 3 channels to 32 channels adds 4736 parameters. An increase in the number of layers in the interest of experimentation leads to an equal increase in complexity for training the model. Training then requires greater computational power and memory.\nMore expressive, less different: A neural network is often considered to be a function approximator. It has the ability to model functions given input, target and a comparison between the function output and target. Adding multiple layers into a network makes it more capable to model complex functions. But results published in the paper stated that a 20-layer plain neural network performs considerably better than a 56-layer plain neural network as can be seen in the below graph.\n\nAdding layers can be seen as an expansion of the function space. For example, multiple layers added together can be seen as a function \\(F\\). This function \\(F\\) can be expressed as a representation of a function space \\(F`\\) that it can reach/model.\nHaving your desirable function in \\(F'\\) would be a lucky chance, but more often than not, it is not the case. Adding layers here allows us to expand and change around the function space \\(F'\\), allowing us to cover a larger space in the larger parent function space consisting of all possible functions in the conceivable universe. But this method has an inherent pitfall. As the function space becomes larger, there is no guarantee that we get closer to our target function. In fact, there is a good chance that in the experimental phase, you move away from the function space that may have the function you actually need.\n\nDid the jargon confuse you? Let’s take an analogy of a needle and a haystack.\nLet the needle be the perfect weights of the neural network, or as explained before, a function. Let the haystack be all the possible functions that can be made.\nOne starts from a single search area and tries to zero into the needle from there. Adding layers is equivalent to moving your search area and making it bigger. But that comes with the risk of moving away from the place where the needle actually is as well as making our search more time-consuming and difficult. Larger the haystack, more difficult it is to find the perfect needle. What is the solution, then?\n\nQuite simple and elegant, actually. Nest your function spaces.\nThis is done for a few simple reasons. The most important one being the fact that it allows you to ensure that while the model adds layers to increase the size of the function space, you don’t end up degrading the model. This gives the guarantee that while our model can do better with more layers, it will not do any worse.\nComing back to our haystack analogy, this is equivalent to making our search space larger, but making sure that we do not move away from our current search space.\n\nVanishing/Exploding Gradient: This is one of the most common problems plaguing the training of larger/deep neural networks and is a result of oversight in terms of numerical stability of the network’s parameters.\nDuring backpropagation, as we keep moving from the deep to the shallow layers, the chain rule of differentiation makes us multiply the gradients. Often, these gradients are small, to the order of \\(10^{-5}\\) or more. According to some simple math, as these small numbers keep getting multiplied with each other, they keep becoming infinitesimally smaller, making almost negligible changes to the weights.\nOn the other end of the spectrum, there are cases when the gradient reaches orders upto \\(10^{4}\\) and more. As these large gradients multiply with each other, the values tend to move towards infinity. Allowing such a large range of values to be in the numerical domain for weights makes convergence difficult to achieve.\nThis problem is popularly known as the Vanishing/Exploding gradient problem. ResNet, due to its architecture, does not allow these problems to occur at all. How so? The skip connections (described ahead) do not allow it as they act as gradient super-highways, allowing it to flow without being altered by a large magnitude."
  },
  {
    "objectID": "posts/2022-08-05-the-annotated-resnet.html#what-are-skip-connections",
    "href": "posts/2022-08-05-the-annotated-resnet.html#what-are-skip-connections",
    "title": "The Annotated ResNet-50",
    "section": "What are Skip Connections?",
    "text": "What are Skip Connections?\nThe ResNet paper popularized the approach of using Skip Connections. If you recall, the approach to solving our function space problems was to nest them. In terms of applying it to our use-case, it was the introduction of a simple addition of the identity function to the output.\nIn mathematical terms, it would mean \\(y = x + F(x)\\) where y is the final output of the layer.\n\nIn terms of architecture, if any layer ends up damaging the performance of the model in a plain network, it gets skipped due to the presence of the skip-connections"
  },
  {
    "objectID": "posts/2022-08-05-the-annotated-resnet.html#architecture",
    "href": "posts/2022-08-05-the-annotated-resnet.html#architecture",
    "title": "The Annotated ResNet-50",
    "section": "Architecture",
    "text": "Architecture\n\nThe ResNet-50 architecture can be broken down into 6 parts 1. Input Pre-processing 2. Cfg[0] blocks 3. Cfg[1] blocks 4. Cfg[2] blocks 5. Cfg[3] blocks 6. Fully-connected layer\nDifferent versions of the ResNet architecture use a varying number of Cfg blocks at different levels, as mentioned in the figure above. A detailed, informative listing can be found below"
  },
  {
    "objectID": "posts/2022-08-05-the-annotated-resnet.html#show-me-the-code",
    "href": "posts/2022-08-05-the-annotated-resnet.html#show-me-the-code",
    "title": "The Annotated ResNet-50",
    "section": "Show me the code!",
    "text": "Show me the code!\nThe best way to understand the concept is through some code. The implementation below is done in Keras, uses the standard ResNet-50 architecture (ResNet has several versions, differing in the depth of the network). We will train the model on the famous Stanford Dogs dataset by Stanford AI\n\nImport headers\n\n\nCode\n!pip install -q tfds\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_datasets as tfds\nimport os\nimport PIL\nimport pathlib\nimport PIL.Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom datetime import datetime\n\n\n\n\nDataset download and pre-processing\nWe download the Stanford Dogs dataset using Tensorflow Datasets (stable) and split it into a training, validation and test set.\nAlong with the images and labels, we also get some meta-data which gives us more information about the dataset. That is stored in ds_info and printed in a human-readable manner.\nWe also make use of tfds.show_examples() to print some random example images and labels from the dataset.\nWe run tfds.benchmark() to perform a benchmarking test on the iterator provided by tf.data.Dataset\nWe perform the following best-practice steps on the tf.data.Dataset object to make it efficient: - batch(BATCH_SIZE) : Allows us to prepare mini-batches within the dataset. Note that the batching operation requires all images to be of the same size and have the same number of channels - map(format_image) : Cast the image into a tf.float32 Tensor, normalize all values in the range \\([0,1]\\), resize the image from its original shape to the model-input shape of \\((224, 224, 3)\\) using the lanczos3 kernel method - prefetch(BUFFER_SIZE) : Pre-fetch brings in the next batch of the dataset during training into memory while the current batch is being processed, reducing the I/O time but requiring more memory in the GPU - cache() : Caches the first batch of the iterator to reduce load-times, similar to prefetch with the difference simply being that cache will load the files but not push into GPU memory\n\n\nCode\n(train_ds, valid_ds, test_ds), ds_info = tfds.load(\n    'stanford_dogs', \n    split=['train', 'test[0%:10%]', 'test[10%:]'], \n    shuffle_files=True, with_info=True,\n    as_supervised=True\n)\n\nprint(\"Dataset info: \\n\")\nprint(f'Name: {ds_info.name}\\n')\nprint(f'Number of training samples : {ds_info.splits[\"train\"].num_examples}\\n')\nprint(f'Number of training samples : {ds_info.splits[\"test\"].num_examples}\\n')\nprint(f'Description : {ds_info.description}')\ntfds.show_examples(train_ds, ds_info)\n\nCLASS_TYPES = ds_info.features['label'].num_classes\nBATCH_SIZE = 4\n\nprint('Benchmark results')\ntfds.benchmark(train_ds)\n\ndef format_image(image, label):\n\n    image = tf.cast(image, tf.float32)\n    image = image / 255.0\n    image = tf.image.resize_with_pad(image, 224, 224, method='lanczos3', antialias=True)\n    return image, label\n\ndef prepare_ds(ds):\n    ds = ds.map(format_image)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n    ds = ds.cache()\n    return ds\n\ntrain_ds = prepare_ds(train_ds)\nvalid_ds = prepare_ds(valid_ds)\ntest_ds = prepare_ds(test_ds)\n\n\nDownloading and preparing dataset 778.12 MiB (download: 778.12 MiB, generated: Unknown size, total: 778.12 MiB) to /root/tensorflow_datasets/stanford_dogs/0.2.0...\nDataset stanford_dogs downloaded and prepared to /root/tensorflow_datasets/stanford_dogs/0.2.0. Subsequent calls will reuse this data.\nDataset info: \n\nName: stanford_dogs\n\nNumber of training samples : 12000\n\nNumber of training samples : 8580\n\nDescription : The Stanford Dogs dataset contains images of 120 breeds of dogs from around\nthe world. This dataset has been built using images and annotation from\nImageNet for the task of fine-grained image categorization. There are\n20,580 images, out of which 12,000 are used for training and 8580 for\ntesting. Class labels and bounding box annotations are provided\nfor all the 12,000 images.\nBenchmark results\n\n************ Summary ************\n\nExamples/sec (First included) 787.00 ex/sec (total: 12000 ex, 15.25 sec)\nExamples/sec (First only) 10.34 ex/sec (total: 1 ex, 0.10 sec)\nExamples/sec (First excluded) 791.95 ex/sec (total: 11999 ex, 15.15 sec)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAugmentation\nWe perform some data augmentation to allow our model to be more robust. A RandomFlip, RandomRotation and RandomContrast is used to make the image set more varied. The parameters to the functions are probabilities, i.e. the chance that an image will undergo the selected transformation\n\n\nCode\nimageAug = keras.Sequential([\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n    keras.layers.RandomRotation(0.2),\n    keras.layers.RandomContrast(0.2)\n])\n\n\n\n\nCfg0 Block\nThis block contains 1 Conv Layer and 2 Identity Layers. For helping numerical stability, we specify a kernel constraint which makes sure that all weights are normalized at constant intervals. Between 2 subsequent layers, we also include a BatchNormalization layer. The code has been written in an explicit way deliberately to help readers understand what design choices have been made at each stage\n\nInput Shape : \\((56, 56, 64)\\)\nOutput Shape : \\((56, 56, 256)\\)\n\n\n\nCode\ncfg0_conv_input = keras.Input(shape=(56,56,64), name='cfg0_conv')\nx = keras.layers.Conv2D(64, kernel_size=1, strides=1, activation='relu', padding='valid', kernel_constraint=keras.constraints.max_norm(2.)) (cfg0_conv_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(64, kernel_size=3, strides=1, activation='relu', padding='same', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(256, kernel_size=1, strides=1, padding='valid', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\n\ncfg0_conv_input_transform = keras.layers.Conv2D(256, kernel_size=1, strides=1, activation='relu', padding='same', kernel_constraint=keras.constraints.max_norm(2.)) (cfg0_conv_input)\ncfg0_conv_input_op = keras.layers.BatchNormalization()(cfg0_conv_input_transform)\nx = keras.layers.Add()([x, cfg0_conv_input_op])\ncfg0_conv_output = keras.layers.ReLU()(x)\n\ncfg0_conv = keras.Model(inputs=cfg0_conv_input, outputs=cfg0_conv_output, name='cfg0_conv')\n\n\n\n\ncfg0_identity_input = keras.Input(shape=(56, 56, 256), name='cfg0_identity')\nx = keras.layers.Conv2D(64, kernel_size=1, strides=1, padding='valid', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (cfg0_identity_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(64, kernel_size=3, strides=1, padding='same', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(256, kernel_size=1, strides=1, padding='valid', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\n\nx = keras.layers.Add()([x, cfg0_identity_input])\ncfg0_identity_output = keras.layers.ReLU()(x)\n\ncfg0_identity = keras.Model(inputs=cfg0_identity_input, outputs=cfg0_identity_output, name='cfg0_identity_p1')\n\n\n\n\ncfg0_input = keras.Input(shape=(56, 56, 64), name='cfg0')\nx = cfg0_conv(cfg0_input)\nx = cfg0_identity(x)\ncfg0_output = cfg0_identity(x)\n\ncfg0 = keras.Model(inputs=cfg0_input, outputs=cfg0_output, name='cfg0_block')\n\n\n\n\nCfg1 Block\nThis block contains 1 Conv Layer and 2 Identity Layers. This is similar to the Cfg0 blocks, with the difference mainly being in the number of out_channels in the Conv and Identity layers being more.\n\nInput Shape : \\((56, 56, 256)\\)\nOutput Shape : \\((28, 28, 512)\\)\n\n\n\nCode\ncfg1_conv_input = keras.Input(shape=(56, 56, 256), name='cfg1_conv')\nx = keras.layers.Conv2D(128, kernel_size=1, strides=2, activation='relu', padding='valid', kernel_constraint=keras.constraints.max_norm(2.)) (cfg1_conv_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(128, kernel_size=3, strides=1, activation='relu', padding='same', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(512, kernel_size=1, strides=1, padding='valid', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\n\ncfg1_conv_input_transform = keras.layers.Conv2D(512, kernel_size=1, strides=2, activation='relu', padding='same', kernel_constraint=keras.constraints.max_norm(2.)) (cfg1_conv_input)\ncfg1_conv_input_output = keras.layers.BatchNormalization()(cfg1_conv_input_transform)\nx = keras.layers.Add()([x, cfg1_conv_input_output])\ncfg1_conv_output = keras.layers.ReLU()(x)\n\ncfg1_conv = keras.Model(inputs=cfg1_conv_input, outputs=cfg1_conv_output, name='cfg1_conv')\n\n\n\n\ncfg1_identity_input = keras.Input(shape=(28, 28, 512), name='cfg1_identity')\nx = keras.layers.Conv2D(128, kernel_size=1, strides=1, padding='valid', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (cfg1_identity_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(512, kernel_size=1, strides=1, padding='valid', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\n\nx = keras.layers.Add()([x, cfg1_identity_input])\ncfg1_identity_output = keras.layers.ReLU()(x)\n\ncfg1_identity = keras.Model(inputs=cfg1_identity_input, outputs=cfg1_identity_output, name='cfg1_identity_p1')\n\n\ncfg1_input = keras.Input(shape=(56, 56, 256), name='cfg1')\nx = cfg1_conv(cfg1_input)\nx = cfg1_identity(x)\nx = cfg1_identity(x)\ncfg1_output = cfg1_identity(x)\n\ncfg1 = keras.Model(inputs=cfg1_input, outputs=cfg1_output, name='cfg1_block')\n\n\n\n\nCfg2 Block\nThis block contains 1 Conv layer and 5 Identity layers. This is one of the more important blocks for ResNet as most versions of the model differ in this block-space.\n\nInput Shape : \\((28, 28, 512)\\)\nOutput Shape : \\((14, 14, 1024)\\)\n\n\n\nCode\ncfg2_conv_input = keras.Input(shape=(28, 28, 512), name='cfg2_conv')\nx = keras.layers.Conv2D(256, kernel_size=1, strides=2, activation='relu', padding='valid', kernel_constraint=keras.constraints.max_norm(2.)) (cfg2_conv_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(256, kernel_size=3, strides=1, activation='relu', padding='same', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(1024, kernel_size=1, strides=1, padding='valid', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\n\ncfg2_conv_input_transform = keras.layers.Conv2D(1024, kernel_size=1, strides=2, activation='relu', padding='same', kernel_constraint=keras.constraints.max_norm(2.)) (cfg2_conv_input)\ncfg2_conv_input_output = keras.layers.BatchNormalization()(cfg2_conv_input_transform)\nx = keras.layers.Add()([x, cfg2_conv_input_output])\ncfg2_conv_output = keras.layers.ReLU()(x)\n\ncfg2_conv = keras.Model(inputs=cfg2_conv_input, outputs=cfg2_conv_output, name='cfg2_conv')\n\n\n\n\ncfg2_identity_input = keras.Input(shape=(14, 14, 1024), name='cfg2_identity')\nx = keras.layers.Conv2D(256, kernel_size=1, strides=1, padding='valid', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (cfg2_identity_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(256, kernel_size=3, strides=1, padding='same', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(1024, kernel_size=1, strides=1, padding='valid', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\n\nx = keras.layers.Add()([x, cfg2_identity_input])\ncfg2_identity_output = keras.layers.ReLU()(x)\n\ncfg2_identity = keras.Model(inputs=cfg2_identity_input, outputs=cfg2_identity_output, name='cfg2_identity_p1')\n\n\ncfg2_input = keras.Input(shape=(28, 28, 512), name='cfg2')\nx = cfg2_conv(cfg2_input)\nx = cfg2_identity(x)\nx = cfg2_identity(x)\nx = cfg2_identity(x)\nx = cfg2_identity(x)\ncfg2_output = cfg2_identity(x)\n\ncfg2 = keras.Model(inputs=cfg2_input, outputs=cfg2_output, name='cfg2_block')\n\n\n\n\nCfg3 Block\nThis block contains 1 Conv Layer and 2 Identity Layers. This is the last set of Convolutional Layer blocks present in the network.\n\nInput Shape : \\((14, 14, 1024)\\)\nOutput Shape : \\((7, 7, 2048)\\)\n\n\n\nCode\ncfg3_conv_input = keras.Input(shape=(14, 14, 1024), name='cfg3_conv')\nx = keras.layers.Conv2D(512, kernel_size=1, strides=2, activation='relu', padding='valid', kernel_constraint=keras.constraints.max_norm(2.)) (cfg3_conv_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(512, kernel_size=3, strides=1, activation='relu', padding='same', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(2048, kernel_size=1, strides=1, padding='valid', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\n\ncfg3_conv_input_transform = keras.layers.Conv2D(2048, kernel_size=1, strides=2, activation='relu', padding='same', kernel_constraint=keras.constraints.max_norm(2.)) (cfg3_conv_input)\ncfg3_conv_input_output = keras.layers.BatchNormalization()(cfg3_conv_input_transform)\nx = keras.layers.Add()([x, cfg3_conv_input_output])\ncfg3_conv_output = keras.layers.ReLU()(x)\n\ncfg3_conv = keras.Model(inputs=cfg3_conv_input, outputs=cfg3_conv_output, name='cfg3_conv')\n\n\n\n\ncfg3_identity_input = keras.Input(shape=(7, 7, 2048), name='cfg3_identity')\nx = keras.layers.Conv2D(512, kernel_size=1, strides=1, padding='valid', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (cfg3_identity_input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(512, kernel_size=3, strides=1, padding='same', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.Conv2D(2048, kernel_size=1, strides=1, padding='valid', activation='relu', kernel_constraint=keras.constraints.max_norm(2.)) (x)\nx = keras.layers.BatchNormalization()(x)\n\nx = keras.layers.Add()([x, cfg3_identity_input])\ncfg3_identity_output = keras.layers.ReLU()(x)\n\ncfg3_identity = keras.Model(inputs=cfg3_identity_input, outputs=cfg3_identity_output, name='cfg3_identity_p1')\n\n\ncfg3_input = keras.Input(shape=(14, 14, 1024), name='cfg3')\nx = cfg3_conv(cfg3_input)\nx = cfg3_identity(x)\ncfg3_output = cfg3_identity(x)\n\ncfg3 = keras.Model(inputs=cfg3_input, outputs=cfg3_output, name='cfg3_block')\n\n\n\n\nClassifier Block\nThis block contains an AveragePooling Layer, a Dropout Layer and a Flatten layer. At this block, the feature map is finally flattened and pushed into a Fully Connected Layer which is then used for producing predictions. A Softmax activation is applied to generate logits/probabilities.\n\nInput Shape : \\((7, 7, 2048)\\)\nOutput Shape : \\(( 1,\\) CLASS_TYPES \\()\\)\n\n\n\nCode\nclassifier_input = keras.Input(shape=(7, 7, 2048), name='classifier')\nx = keras.layers.AveragePooling2D(pool_size=2, padding='same')(classifier_input)\nx = keras.layers.Dropout(0.2)(x)\nx = keras.layers.Flatten()(x)\nclassifier_output = keras.layers.Dense(CLASS_TYPES, activation='softmax', kernel_constraint=keras.constraints.max_norm(2.))(x)\n\nclassifier = keras.Model(inputs=classifier_input, outputs=classifier_output, name='classifier')\n\n\n\n\nBuild ResNet Model\nNow we take all the blocks and join them together to create the final ResNet Model. In our entire process, we have used the Keras Functional API, which is a best-practice for Tensorflow\nWe also perform some visualizations, namely model.summary() to print out the structure of the model’s layers and keras.utils.plot_model() to plot the visualized Directed Acyclic Graph of the model that will be used by Tensorflow in the backend to streamline execution\n\n\nCode\ndef build_resnet_model():\n    resnet_input = keras.Input(shape=(224, 224, 3), name='input')\n    x = imageAug(resnet_input)\n    x = keras.layers.Conv2D(64, kernel_size=7, activation='relu', padding='same', strides=2, kernel_constraint=keras.constraints.max_norm(2.))(x)\n    conv1_output = keras.layers.MaxPooling2D(pool_size=3, padding='same', strides=2) (x)\n    x = cfg0(conv1_output)\n    x = cfg1(x)\n    x = cfg2(x)\n    x = cfg3(x)\n    model_output = classifier(x)\n    resnet_model = keras.Model(inputs=resnet_input, outputs=model_output, name='resnet50')\n\n    print(resnet_model.summary())\n\n    resnet_model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=['accuracy'],\n    )\n    \n    return resnet_model\nmodel = build_resnet_model()\n\n\nkeras.utils.plot_model(model, show_shapes=True, rankdir='TB', show_layer_activations=True, expand_nested=True)\n\n\nModel: \"resnet50\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input (InputLayer)          [(None, 224, 224, 3)]     0         \n                                                                 \n sequential (Sequential)     (None, 224, 224, 3)       0         \n                                                                 \n conv2d_28 (Conv2D)          (None, 112, 112, 64)      9472      \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 56, 56, 64)       0         \n )                                                               \n                                                                 \n cfg0_block (Functional)     (None, 56, 56, 256)       148480    \n                                                                 \n cfg1_block (Functional)     (None, 28, 28, 512)       665600    \n                                                                 \n cfg2_block (Functional)     (None, 14, 14, 1024)      2641920   \n                                                                 \n cfg3_block (Functional)     (None, 7, 7, 2048)        10526720  \n                                                                 \n classifier (Functional)     (None, 120)               3932280   \n                                                                 \n=================================================================\nTotal params: 17,924,472\nTrainable params: 17,893,752\nNon-trainable params: 30,720\n_________________________________________________________________\nNone\n\n\n\n\nDefining Callbacks\nIn model.fit(), we can define callbacks for the model that are invoked during training at pre-determined intervals. We define a Model Checkpoint callback that creates a snapshot of the model at the completion of each epoch.\n\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='resnet50_model/checkpoint_{epoch:02d}.hdf5',\n        monitor='val_loss',\n        verbose=0,\n        save_best_only=True,\n        mode='auto',\n        save_freq='epoch',\n        options=None,\n        initial_value_threshold=None\n    )\n]\n\nhistory = model.fit(\n    x=train_ds,\n    validation_data=valid_ds,\n    callbacks=callbacks_list,\n    epochs=20\n)\n\n\n##collapse_show\n## If using Google Colaboratory, one can upload checkpoints onto Google Drive and use it directly.\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\nmodel = keras.models.load_model('/content/gdrive/My Drive/checkpoint_18.hdf5')\n\n## If using local Jupyter Notebooks, one can use checkpoints from local drives itself.\nmodel = keras.models.load_model('./resnet50_model/checkpoint_18.hdf5')\n\n\n\nGet model history\nWe print the model history to get more information about the training process\n\nprint(history)\n\n\n\nPredicting results\nWe take the trained model and use it to perform predictions on the test set as well as calculate several metrics like Loss and Accuracy\n\nresults = model.evaluate(test_ds)\nprint(f\"Results : {results}\")"
  },
  {
    "objectID": "posts/2022-08-05-the-annotated-resnet.html#conclusion",
    "href": "posts/2022-08-05-the-annotated-resnet.html#conclusion",
    "title": "The Annotated ResNet-50",
    "section": "Conclusion",
    "text": "Conclusion\nAbove, we have visited the Residual Network architecture, gone over its salient features, implemented a ResNet-50 model from scratch and trained it to get inferences on the Stanford Dogs dataset.\nAs a model, ResNet brought about a revolution in the field of Computer Vision and Deep Learning simultaneously. It went on to win the ImageNet Large Scale Visual Recognition Challenge of 2015 and COCO Competition. But it was only a stepping stone to many interesting variations which yielded better results. Check the Interesting Links section below to find some great blogs and research papers for the same."
  },
  {
    "objectID": "posts/2022-08-05-the-annotated-resnet.html#references",
    "href": "posts/2022-08-05-the-annotated-resnet.html#references",
    "title": "The Annotated ResNet-50",
    "section": "References",
    "text": "References\n{% bibliography –cited %}"
  },
  {
    "objectID": "posts/2022-08-05-the-annotated-resnet.html#interesting-links",
    "href": "posts/2022-08-05-the-annotated-resnet.html#interesting-links",
    "title": "The Annotated ResNet-50",
    "section": "Interesting Links",
    "text": "Interesting Links\n\nAn overview of ResNets and its variants\nPaper on Multi-scale ensemble of ResNet variants\nTraining a ResNet-50 on a Cloud TPU"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html",
    "title": "How to make your models available to the public",
    "section": "",
    "text": "An end-to-end Machine Learning solution is an important way to bring AI to production and make it available for mass consumption and usage. But today, most AI practitioners simply do the pre-processing, training, evaluation and tuning stages and leave the remaining part to DevOps engineers.\nAs such, a new field of development named MLOps has come into the mainstream. The focus has shifted from simply training and evaluation to also bringing and integrating it to production pipelines.\nOn an individual level as well, knowing how to bring your model to the public is an important tool to have in an AI practitioner’s skill-set.\nIn this article, we will be exploring how we can perform a small segment of the MLOps cycle in a simple and efficient manner using Keras, Flask, Gunicorn and Docker.\nIf you wish to skip through and go straight to the code, click here to go to the GitHub repository\n\n\n\nimage"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#introduction",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#introduction",
    "title": "How to make your models available to the public",
    "section": "",
    "text": "An end-to-end Machine Learning solution is an important way to bring AI to production and make it available for mass consumption and usage. But today, most AI practitioners simply do the pre-processing, training, evaluation and tuning stages and leave the remaining part to DevOps engineers.\nAs such, a new field of development named MLOps has come into the mainstream. The focus has shifted from simply training and evaluation to also bringing and integrating it to production pipelines.\nOn an individual level as well, knowing how to bring your model to the public is an important tool to have in an AI practitioner’s skill-set.\nIn this article, we will be exploring how we can perform a small segment of the MLOps cycle in a simple and efficient manner using Keras, Flask, Gunicorn and Docker.\nIf you wish to skip through and go straight to the code, click here to go to the GitHub repository\n\n\n\nimage"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#what-is-covered-in-this-tutorial",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#what-is-covered-in-this-tutorial",
    "title": "How to make your models available to the public",
    "section": "What is covered in this tutorial?",
    "text": "What is covered in this tutorial?\n\nCreate a custom model using Keras and its off-the-shelf components\n\nPrepare an inference pipeline\n\nDevelop a simple Flask app to expose the model for inference\n\nDefine a Dockerfile using Gunicorn\n\nBuild our image\nDefine a simple Github Actions workflow to build your image every time you push it to your repository"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#create-a-custom-model-using-keras",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#create-a-custom-model-using-keras",
    "title": "How to make your models available to the public",
    "section": "1) Create a custom model using Keras",
    "text": "1) Create a custom model using Keras\nAs an example, we are going to create a simple model using the Keras Functional API and an off-the-shelf MobileNetV2 model from keras.applications pretrained on ImageNet.\n\nImport headers\nWe require tensorflow, keras, Flask, PIL and os for this tutorial. If using a virtual environment, you can use the requirements.txt file below to get your env prepared.\n\ntensorflow: Used for matrix operations and back-end for keras\nkeras: Used for high-level Deep Learning model-building API and get pre-trained model\nFlask: Used for building simple API for inference\nPIL: Used for handling images\nos: Used for setting environment variables\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom flask import Flask\nfrom flask import request, jsonify\nfrom PIL import Image\nimport os\n\n\n\nSet options\nSince GPUs are a difficult resource to get a hold of, we set a Tensorflow flag to make any CUDA devices present invisible in the first place. If you can run your container on a GPU, feel free to skip this line.\n\n# To force inference using CPU only\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\n\n\nModel definition\nThis model is made using the Keras Functional API. We take a simple keras.Input which accepts color (RGB) images of any size.\nThe input is passed via the following layers:\n- keras.layers.Resizing : Used to resize the image tensor to a 224x224x3 tensor - keras.layers.Rescaling : Used to rescale the image tensor values from a [0,255] range to a [0,1] range - keras.applications.MobileNetV2 : Used to import the MobileNetV2 instance from Keras (pretrained on ImageNet)\n\nimage_input = keras.Input(shape=(None,None,3))\n\nx = keras.layers.Resizing(height=224, width=224, interpolation='lanczos3', crop_to_aspect_ratio=False)(image_input)\n\nx = keras.layers.Rescaling(scale=1./255, offset=0.0)(x)\n\nmobilenet = keras.applications.MobileNetV2(\n    alpha=1.0,\n    include_top=True,\n    weights=\"imagenet\",\n    input_tensor=image_input,\n    classes=1000,\n    classifier_activation=\"softmax\"\n)\n\nmodel_output = mobilenet(x)\n\nmodel = keras.Model(inputs=image_input, outputs=model_output)\n\n\n\nRequirements file\nGunicorn is used to deploy the API on several workers together to allow lower latency at the expense of increased compute consumption. Gunicorn is used since it implements WSGI. In a production environment, a front-facing server like NGINX or Apache Web Server is used to host Static web pages and load balancers with Gunicorn running behind this layer to enable functionality.\n\nFlask==2.0.3\nPillow==9.2.0\ntensorflow==2.9.1\ngunicorn==20.1.0"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#prepare-an-inference-pipeline",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#prepare-an-inference-pipeline",
    "title": "How to make your models available to the public",
    "section": "2) Prepare an inference pipeline",
    "text": "2) Prepare an inference pipeline\nWe define a simple function which accepts a tf.Tensor and runs it through the model to return a final top-5 predictions dictionary result.\n\nInference function\nThe image, accepted as a tf.Tensor, is inferred using the function prepared before. The numpy value of the tensor is then extracted to get all the confidence scores for each class.\nThis numpy array is then passed into keras.applications.imagenet_utils.decode_predictions to get the top 5 predictions.\n\ndef inference(image: tf.Tensor):\n    y = model(image).numpy()\n    preds = keras.applications.imagenet_utils.decode_predictions(y, top=5)\n    result = {i[1] : str(i[2]) for i in preds[0]}\n    result = {k: v for k, v in sorted(result.items(), key=lambda item: item[1])}\n    return result"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#make-a-simple-flask-app-to-expose-model-for-inference",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#make-a-simple-flask-app-to-expose-model-for-inference",
    "title": "How to make your models available to the public",
    "section": "3) Make a simple Flask App to expose model for inference",
    "text": "3) Make a simple Flask App to expose model for inference\nNow, we define 2 simple endpoints at the routes / and /inference.\n- / (GET) : The first endpoint acts as a health-check to make sure that the API is up and running\n- /inference (POST) : The second endpoint accepts an image as a form field with the parameter name image and returns a dictionary with the confidence scores and the ImageNet class names\n\nFlask App definition\napp is the name of the WSGI callable that will be used by Gunicorn later on. To know more about what WSGI is, check the Interesting Links section below.\n\napp = Flask(__name__)  \n\n\n\nDefinition of health-check endpoint\nTo test whether the API is up and running, we simply hit a GET request on this endpoint to get the expected output.\n\n@app.route(\"/\", methods=['GET'])\ndef health_check():\n    result = {\n        'outcome':'endpoint working successfully'\n    }\n    return jsonify(result)\n\n\n\nDefinition of inference endpoint\nHere, we accept a POST request, extract the image parameter from the files sent in the request. This is stored in a file-stream format which is then passed into a PIL.Image.open to prepare the image. Finally, we perform some simple pre-processing to convert the PIL image to a tf.Tensor and prepare a batch of 1 image to be passed into our inference function. The result returned is then passed into jsonify for response preparation and execution\n\n@app.route(\"/inference\", methods=['POST'])\ndef perform_inference():\n    image = request.files['image']\n    pil_img = Image.open(image.stream)\n    tensor = keras.preprocessing.image.img_to_array(pil_img)\n    tensor = tf.expand_dims(tensor, axis=0)\n    result = inference(tensor)\n    return jsonify(result)"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#define-a-dockerfile-which-uses-gunicorn-for-deployment",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#define-a-dockerfile-which-uses-gunicorn-for-deployment",
    "title": "How to make your models available to the public",
    "section": "4) Define a Dockerfile which uses Gunicorn for deployment",
    "text": "4) Define a Dockerfile which uses Gunicorn for deployment\nWe are now done with defining our model and preparing it for inference using a simple Flask App. Here, we begin writing a Dockerfile and a .dockerignore to build a custom Docker Image\n\nFROM ubuntu:20.04\n\nRUN apt-get update && apt-get install -y \\\ngit \\\ncurl \\\nca-certificates \\\npython3 \\\npython3-pip \\\nsudo \\\n&& rm -rf /var/lib/apt/lists/*\n\nRUN useradd -m docker_runner\n\nRUN chown -R docker_runner:docker_runner /home/docker_runner\n\nCOPY --chown=docker_runner *.* /home/docker_runner/flask_app/keras-docker-trial/\n\nUSER docker_runner\n\nWORKDIR /home/docker_runner/flask_app/keras-docker-trial\n\nENV PATH=\"${PATH}:/home/docker_runner/.local/bin\"\n\nRUN pip install --no-cache-dir -r requirements.txt\n\nENTRYPOINT [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"--workers=4\", \"app:app\"]\n\nEXPOSE 5000\n\n\nDockerfile\n\nThe first line pulls the ubuntu:20.04 image from Docker Hub to prepare a container with stock Ubuntu 20.04 Focal Fossa within it.\n\nThe first RUN command downloads and installs several essential packages that we require later ahead.\nThe next RUN command adds a user named docker_runner and creates a home directory for the user (using the -m option)\nThe next RUN command changes directory ownership and assigns docker_runner as the owner of its own home directory in a recursive manner for all files and subdirectories as well (using the -R option)\nThe COPY command moves all the files present in the current repository where the Dockerfile is into the container’s target directory\nThe USER command is used to change the current active user to docker_runner\nThe WORKDIR command is used to change the current active directory to /home/docker_runner/flask_app/keras-docker-trial\nThe ENV command is used to set the PATH environment variable and add our user’s /.local/bin directory to it\nThe RUN command is now used to install all the requirements and not use any cached directories or their SHA hashes while doing so\nThe ENTRYPOINT command is used to begin the API deployment using gunicorn. We bind the localhost’s port 5000 and start up 4 workers for this task. We specify the WSGI callable as app on the left side of app:app. If you changed the name of the Flask app in Step 3, then you should change this part as {your_app_name}:app\nThe EXPOSE command is used to make the container listen on port 5000\n\n\n\n.dockerignore\nWe just ignore the __pycache__/ directory as it generates intermediate files from CPython\n\n__pycache__/"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#build-our-image",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#build-our-image",
    "title": "How to make your models available to the public",
    "section": "5) Build our image",
    "text": "5) Build our image\nWe now build our image and assign it a tag keras-docker-trial.\n\ndocker build . -t keras-docker-trial --file Dockerfile"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#define-a-simple-github-actions-workflow-to-build-your-image-every-time-you-push-it-to-your-repository",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#define-a-simple-github-actions-workflow-to-build-your-image-every-time-you-push-it-to-your-repository",
    "title": "How to make your models available to the public",
    "section": "6) Define a simple GitHub Actions workflow to build your image every time you push it to your repository",
    "text": "6) Define a simple GitHub Actions workflow to build your image every time you push it to your repository\nHere, as an extra step, we use GitHub Actions to build our image as a test every time a Push is made to any branch or if a PR is merged in the repository. This needs to be added only if you are preparing a repository on GitHub for your model.\n\nname : Assigns a name to the workflow\non : Defines the triggers for when the workflow is to be used\nenv : Sets environment variables\njobs : Defines the different commands and workflow actions to be run as part of the current workflow\nruns-on : Defines which GitHub-hosted runner is used for execution of workflow\nactions/checkout@v3 : Used to check-out the code from repository\nBuild Docker Image : Build image from Dockerfile present in repository\n\n\nname: Docker CI\n\non:\n  push:\n  pull_request:\n    types: ['opened', 'reopened']\n\nenv:\n  BUILD_CONFIGURATION: Release\n\njobs:\n  job1:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check-out the pushed code\n        uses: actions/checkout@v3\n\n      - name: Build Docker image\n        run: docker build . -t keras-docker-trial --file Dockerfile"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#test-the-pipeline",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#test-the-pipeline",
    "title": "How to make your models available to the public",
    "section": "Test the pipeline",
    "text": "Test the pipeline\nAbove, we have defined the model and deployed it using Docker and Gunicorn. You can find some example screenshots of the deployment and its predictions via Postman API Explorer below.\n\n\n\nimage\n\n\n\nTerminal command\n\n\n\n\nimage\n\n\n\nGET request on health-check\n\n\n\n\nimage\n\n\n\nGET request on inference\n\n\n\n\nimage\n\n\n\nThe Goldfish image sent for request via Postman"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#conclusion",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#conclusion",
    "title": "How to make your models available to the public",
    "section": "Conclusion",
    "text": "Conclusion\nAbove, we have completed the development of a simple Keras model, its deployment via Docker and a GitHub Actions workflow for CI(Continuous Integration)."
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#future-scope",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#future-scope",
    "title": "How to make your models available to the public",
    "section": "Future Scope",
    "text": "Future Scope\nThis is only a small part of what can be done as a part of a simple MLOps pipeline. CML (Continuous Machine Learning) and DVC (Data Version Control) are two important concepts that are an integral part of every self-sustaining machine learning workflow and can be explored further. Resources to do so are present in the Interesting Links section."
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#references",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#references",
    "title": "How to make your models available to the public",
    "section": "References",
    "text": "References\n1.) Docker Hub Documentation\n2.) Keras Applications Documentation\n3.) Gunicorn Documentation"
  },
  {
    "objectID": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#interesting-links",
    "href": "posts/2022-09-05-how-to-make-your-models-available-to-the-public.html#interesting-links",
    "title": "How to make your models available to the public",
    "section": "Interesting Links",
    "text": "Interesting Links\n1.) What is CML?\n2.) What is DVC?\n3.) What is WSGI (Web Server Gateway Interface)?\n4.) Detailed blog on What is MLOps?"
  },
  {
    "objectID": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html",
    "href": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html",
    "title": "Accelerating Model Deployment using Transfer Learning and Vertex AI",
    "section": "",
    "text": "As the field of Machine Learning progresses, the requirement for sharing your progress with the world increases exponentially. This holds true especially for early/mid-stage ML startups that are looking to get their product across to potential users as soon as possible while being able to maintain the load in a performant as well as cost-effective way.\nThis includes being able to leverage the latest research, as well as handling a ton of DevOps aspects related to maintaining the services provided. Along with that, preparing and deploying the models that have been developed in-house as a SaaS offering becomes a critical step in the complete pipeline.\nIn this article, I will be giving a brief introduction to how one can make use of pre-trained models available for free on the internet, consume and mold them to a specific use-case and then deploy it using TensorFlow Serving, Docker and Google Cloud’s Vertex AI.\nThis article assumes that you have a working knowledge of how Neural Networks work, how Docker works and what TensorFlow is. If you do not have the pre-requisite knowledge or require a refresher, consider going through the following resources:\n\nHow Docker Works\nWhat is TensorFlow\nHow Neural Networks work\n\nThis article is an addition to the slides, talk, and code presented as part of my presentation at GDG DevFest Raipur 2022 for the topic with the same title.\nYou can find the slides here, and the code here."
  },
  {
    "objectID": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#introduction",
    "href": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#introduction",
    "title": "Accelerating Model Deployment using Transfer Learning and Vertex AI",
    "section": "",
    "text": "As the field of Machine Learning progresses, the requirement for sharing your progress with the world increases exponentially. This holds true especially for early/mid-stage ML startups that are looking to get their product across to potential users as soon as possible while being able to maintain the load in a performant as well as cost-effective way.\nThis includes being able to leverage the latest research, as well as handling a ton of DevOps aspects related to maintaining the services provided. Along with that, preparing and deploying the models that have been developed in-house as a SaaS offering becomes a critical step in the complete pipeline.\nIn this article, I will be giving a brief introduction to how one can make use of pre-trained models available for free on the internet, consume and mold them to a specific use-case and then deploy it using TensorFlow Serving, Docker and Google Cloud’s Vertex AI.\nThis article assumes that you have a working knowledge of how Neural Networks work, how Docker works and what TensorFlow is. If you do not have the pre-requisite knowledge or require a refresher, consider going through the following resources:\n\nHow Docker Works\nWhat is TensorFlow\nHow Neural Networks work\n\nThis article is an addition to the slides, talk, and code presented as part of my presentation at GDG DevFest Raipur 2022 for the topic with the same title.\nYou can find the slides here, and the code here."
  },
  {
    "objectID": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#table-of-contents",
    "href": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#table-of-contents",
    "title": "Accelerating Model Deployment using Transfer Learning and Vertex AI",
    "section": "Table of Contents",
    "text": "Table of Contents\nThis article can be divided into 2 stages.\n\nTraining\n\nStep 1: What are keras.applications and Hugging Face?\nStep 2: How to use pre-trained models?\nExample: Using it to make a customized pre-trained model\n\n\n\nDeployment\n\nStep 3: Use TensorFlow Serving and SavedModel\nStep 4: Export SavedModel to Docker Image\nStep 5: Create Vertex AI Model and Endpoints\nStep 6: Perform Testing and Inference"
  },
  {
    "objectID": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#training-1",
    "href": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#training-1",
    "title": "Accelerating Model Deployment using Transfer Learning and Vertex AI",
    "section": "Training",
    "text": "Training\n\nStep 1: What are keras.applications and Hugging Face?\nPre-trained models are specific Deep-Learning models that have been trained on a certain dataset of a given task and are available freely on the internet. These models have weights for the network hosted in a way that they can be retrieved. If the user requires, they can also fine-tune the weights by initializing the model and training, using the pre-trained model as a starting point compared to using Random initialization.\nThe process of taking pre-trained models and fine-tuning them or using them on a different task than the one they were originally trained for, is known as Transfer Learning.\nBut where do you find these models?\n\nTensorFlow-Keras ecosystem\n\n\nOne of the most mature ecosystems for pre-trained models is available using the keras.applications Module or using TensorFlow Hub. keras.applications contains a total of 38 Vision models pre-trained on the ImageNet dataset by the team behind Keras. They also provide a table that details statistics like the number of parameters within the model as well as benchmarks like inference latency on a general configuration. These models are extensively used and are available free of charge.\nThrough this, you don’t need to just get inferences. You can get everything in between, be it feature-vectors or fine-tuned models, or even just the base model that can perform inference for ImageNet classes.\nIf it seems suitable to your use case, you can even convert these models to TFLite and deploy them to edge devices (provided the number of parameters is under a certain threshold to ensure support).\nAs Keras keeps on adding support for newer models, you can also check out the supplementary packages of KerasCV and KerasNLP. They contain a larger set of more exotic models along with their pre-trained weights. (You can even try out the latest Stable Diffusion model directly from KerasCV). If that doesn’t fulfill your needs either, you can turn to community-supported and provided models made available via TensorFlow Hub. Folks from the community make implementations of different models using TensorFlow/Keras, train them and provide them through this centralized Hub.\n\nHugging Face ecosystem\n\n\n\n\nimage\n\n\nHugging Face is easily one of the largest, if not the largest host of models on the internet. With over 100,000 models available, there is a low chance that your use case cannot be fulfilled by a model from here. These models are contributed by the community, small-scale teams, large-scale enterprises, and everything in between.\nEnterprises like Google, Microsoft, and Facebook also release pre-trained weights for their latest models from their research teams here. This gives you access to cutting-edge models in a simple manner and without much overhead.\nMoreover, you get this access for free (while a paid service exists for accessing better compute).\nThe ecosystem also provides support for ancillary tasks around ML, like Accelerate (for training pipeline setups), Diffusers (for working with Diffusion Models), Datasets (for accessing the latest datasets), and Spaces (for hosting your models) along with many more.\n\n\nStep 2: How to use pre-trained models?\nBelow are code samples for how you can make pre-trained models from each of the ecosystems mentioned above:\n\nkeras.applications - Getting a ResNet-50 model\n\n\nfrom tensorflow.keras.applications.resnet50 import ResNet50, \npreprocess_input, decode_predictions\n\nmodel = ResNet50(weights='imagenet')\n\nx = preprocess_input(image)\n\npreds = model.predict(x)\n\nresults = decode_predictions(preds)\n\nIn the example above, we perform the following operations:\n\nWe perform the necessary imports and get the ResNet50 model and its helper functions (present within the same submodule) from keras.applications into the environment.\nWe initialize the ResNet50 Model with the weights used being the ones from ImageNet pre-training.\nThe keras.applications.ResNet50 model requires all images passed as input to be in a certain format. Hence, we must use the built-in preprocess_input function to make the changes.\nWe perform inference by passing our preprocessed input image to the model.predict() function to get predictions.\nSince the predictions are generated in the form of a confidence score along with the classes (where the class names are label-encoded), we make use of the decode_predictions() function to make sense of the output we received.\n\n\nTensorFlow Hub: Getting the MLP Mixer model for generating Feature vectors\n\n\ninput_layer = keras.Input(shape=(224, 224, 3))\n\nhub_layer = hub.KerasLayer(\n    \"https://tfhub.dev/sayakpaul/mixer_b16_i21k_fe/1\", trainable=True\n)\n\nx = hub_layer(input_layer)\n\noutput_layer = keras.layers.Dense(num_classes, activation=’softmax’)\n\nmodel = keras.Model(inputs=input_layer, outputs=output_layer)\n\nIn the example above, we instantiate a model using the Keras Functional API. In this, we perform the following steps:\n\nDefine a keras.Input layer that acts as an entry-point for the input tensor. Here, we define a shape that we can expect the input tensor to be.\nWe make use of the tensorflow_hub package and import the model we want to use as a hub.KerasLayer that downloads the model architecture and its trained weights for use.\nWe pass the input tensor to the layer we just made to perform a forward pass.\nWe add a keras.layers.Dense layer with the number of units equal to the number of classes, to generate softmax predictions.\nFinally, we initialize a keras.Model instance with the chosen input and output layers in order to create the final model. This is the method of instantiating Functional API-based models for Keras.\n\n\nHugging Face ( transformers ): Getting the ViT model for 224 x 224 x 3 images\n\n\nfrom transformers import ViTForImageClassification, ViTImageProcessor\n\nfeature_extractor = ViTImageProcessor.from_pretrained(\n  \"google/vit-base-patch16-224\"\n)\n\ninputs = feature_extractor(image, return_tensors=\"pt\")\n\nmodel =  ViTForImageClassification.from_pretrained(\n  \"google/vit-base-patch16-224\"\n)\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_label = logits.argmax(-1).item()\n\nIn the example above, we make use of PyTorch instead of TensorFlow. We perform the following steps:\n\nWe perform necessary imports to get the ViT model and its required Image processing helper functions.\nWe instantiate the ViTImageProcessor from the pre-trained weights as provided by the repository owner (in this case, Google).\nWe pass our image into the feature extractor to get our images formatted and ready for inference. ViT models require images to be passed in the form of uniform patches. This specific model requires your image to be of size 224 x 224 x 3 along with each patch being 16 x 16 in size.\nWe instantiate the ViTForImageClassification model and use the pre-trained weights from the same repository.\nWe use torch.no_grad() to make sure that gradients are not calculated for forward passes within the defined scope. This is for generating predictions. The predictions come as confidence scores for all possible classes. We perform an argmax() over the entire output tensor to get the highest resultant class and use it as the final prediction."
  },
  {
    "objectID": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#demo-using-it-to-make-a-customized-pre-trained-model",
    "href": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#demo-using-it-to-make-a-customized-pre-trained-model",
    "title": "Accelerating Model Deployment using Transfer Learning and Vertex AI",
    "section": "Demo: Using it to make a customized pre-trained model",
    "text": "Demo: Using it to make a customized pre-trained model\nNow, we make use of a customized model in the form of a demo that we will use ahead in the latter part of the article as well.\n\n# To force inference using CPU only\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n\n# Model definition\nimage_input = keras.Input(shape=(None, None, 3))\n\n# Defining a resize Layer\nx = keras.layers.Resizing(\n   height=224, width=224, interpolation=\"lanczos3\", crop_to_aspect_ratio=False\n)(image_input)\n\n# Define a Rescaling layer to get image pixel values from [0, 255] to [0, 1)\nx = keras.layers.Rescaling(scale=1.0 / 255, offset=0.0)(x)\n\n# Instantiate a MobileNetV2 instance with pre-trained weights and the \n# Dense classifier being trained for ImageNet classes\nmobilenet = keras.applications.MobileNetV2(\n   alpha=1.0,\n   include_top=True,\n   weights=\"imagenet\",\n   classes=1000,\n   classifier_activation=\"softmax\",\n)\n\n# Passing input tensor for results\nmodel_output = mobilenet(x)\n\n# Make Keras Model\nmodel = keras.Model(inputs=image_input, outputs=model_output)\n\nIn this section, we have explored how we can make use of several different online services to access pre-trained models that allow us to move forward with developing on top of pre-existing work without requiring heavy Compute.\nBut this was only about the development of the solution. How do we actually get it out for the world to use?"
  },
  {
    "objectID": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#deployment-1",
    "href": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#deployment-1",
    "title": "Accelerating Model Deployment using Transfer Learning and Vertex AI",
    "section": "Deployment",
    "text": "Deployment\n\nStep 3: Use TensorFlow Serving and SavedModel\n\n\n\nimage\n\n\nTensorFlow Serving is part of the larger TFX (TensorFlow Extended) ecosystem that helps users develop full production-scale pipelines. TF-Serving is a low-latency, high-throughput system that is flexible enough to allow a large array of options.\nWhile more straightforward solutions exist to deploy models, like using Flask or Django, that would require you to write and maintain a lot of boilerplate along with performing maintenance, error-handling, and edge-case handling for the framework involved. With TF-Serving, all of that is abstracted away.\nAnother important fact is that TF-Serving is fully compatible with containerization, leading to no worries about packages, compatibility, dependency issues, and so on.\nFrom here on forward, the example we have developed previously is what we will continue to use for the deployment experiment.\nBelow, we first write an inference function. This function will be used by TensorFlow Serving to process input and give results.\n\nversion_number = 1\n\n@tf.function(input_signature=[tf.TensorSpec(name=\"image_bytes\", \n  shape=None, dtype=tf.string)])\ndef predict_b64_string(b64str_tensor):\n   img = tf.reshape(b64str_tensor, [])\n\n   img = tf.io.decode_image(img, \n      channels=3, \n      dtype=tf.float16, \n      expand_animations=False\n    )\n\n   tensor = tf.expand_dims(img, axis=0)\n\n   return model.call(tensor)\n\nIn the above snippet, we define version_number for TF-Serving to maintain version control of the different models present together.\nThe tf.function() decorator is perhaps the most important line of code in the above snippet. This decorator will perform a tracing of the function and generate a computation graph for it. As a default, TensorFlow 2.0 defaults to Eager Execution to allow for better debugging. But the most performant way of execution is in the Graph mode. This decorator will trigger the heavy lifting and prepare the function in a language-agnostic definition. Here, we define a tf.TensorSpec that allows us to define the kind of Input Tensor the function should expect. That means we will accept an input consisting of the bytes of the image in the form of a Base64 string.\nWe then perform the simple operations of decoding the image, preparing the tensor by batching it, and performing a forward class to generate results.\n\nmodel.save(\n   f'./mobilenetv2-imagenet-devfest/{version_number}',\n   save_format='tf',\n   include_optimizer=True,\n   overwrite=True,\n   signatures={\n       \"serving_image_b64string\":predict_b64_string\n   }\n)\n\nHere, we call the model.save() a function that is integral to the deployment process. It requires the following arguments:\n\nFile path: TF-Serving requires you to save it in the format of {model_name}/{version_number}.\nSave Format: Setting this to 'tf' will save the model as a Keras SavedModel , helping you to save model metadata, custom functions, weights, optimizer states, and so on. This is required for TF-Serving because the model will be run in Graph mode only.\nInclude Optimizer: Choose whether to include the optimizer state in the saved package\nOverwrite: Choose whether to overwrite old data that may be present in the same file destination\nSignatures: Here, we must define the function that should be used by TF-Serving to create the model endpoint and subsequently use it for inference. Here we give a name to the signature and pass the function object to it in the form of a dictionary\n\n\n\nStep 4: Export SavedModel to Docker Image\n[suvaditya@fedora ~ ]$ docker run -d --name serving_base tensorflow/serving\n\n[suvaditya@fedora ~ ]$ docker cp /absolute/path/to/saved_model/model_name \\\nserving_base:/models/model_name\n\n[suvaditya@fedora ~ ]$ docker commit --change “ENV MODEL_NAME model_name” \\ \nserving_base devfest-mobilenet-demo\n\n[suvaditya@fedora ~ ]$ docker run -t -p 8501:8501 devfest-mobilenet-demo\nThe code snippet above is a series of Docker commands that will perform the following steps:\n\nWe first create a dummy container that is initialized using the tensorflow/serving image present on Docker Hub. We run it in a detached mode.\nWe give a cp instruction that will copy the contents from a source to the target directory for the Dockerfile of the image. Here, we copy the contents of the SavedModel file structure to a path in the container’s file system where TF-Serving expects your model’s files to be.\nWe commit the changes made to the Dockerfile, insert a new Environment variable for the container, and give a new name to this changed Dockerfile as devfest-mobilenet-demo coming from the serving_base container as the base.\nFinally, we run a new container using the image we just created. We expose and forward requests to Port 8501 as TF-Serving is configured to serve REST API requests on this port. If you require, a separate gRPC server is also available on Port 8500 that can be exposed in a similar manner\n\n\n\nStep 5: Create Vertex AI Model and Endpoints\nNote: The following steps require the presence of a Google Cloud Billing-enabled account. GCP offers $300 in credits for a duration of 90 days. To know more, click here.\nNow, we move to Google Cloud Platform and its state-of-the-art Vertex AI offering.\nWhat is Vertex AI?\n\n\n\nimage\n\n\nVertex AI is the official end-to-end solution for all things ML present on GCP. It offers tools to work on each aspect of the full Machine Learning lifecycle.\n\nDataset Collection and Version Control\nLabeling\nPre-processing\nTraining\nDeployment\nModel Version\nLogging & Monitoring\n\n\n\n\nimage\n\n\nWe have already made a Docker Image that serves the model inside a TF-Serving runtime. Our Vertex AI Model deployment flow will consist of the following steps:\n\nDeploy the Docker image we developed into Google Artifact Registry. This is done in order to maintain a central source for all model versions. Google Artifact Registry is the best solution for storing artifacts as it allows multiple different formats like Maven Packages, Docker Images, etc. all in one place, in comparison to the older Google Container Registry.\nImport and create a Vertex AI Model resource from Artifact Registry.\nCreate a Vertex AI Endpoint that will accept the Vertex AI Model\nDeploy the Vertex AI Endpoint and serve it for request-response traffic from the web.\n\n\n\n1. Put image into Artifact Registry\n\n\n\nimage\n\n\n[suvaditya@fedora ~]$ docker tag devfest-mobilenet-demo \\\nus-central1-docker.pkg.dev/test-da9ec/devfest-vertex-ai/devfest-mobilenet-demo\n\n[suvaditya@fedora ~]$ docker push \\\nus-central1-docker.pkg.dev/test-da9ec/devfest-vertex-ai/devfest-mobilenet-demo\nHere, we first go to the Artifact Registry screen and create a repository. We configure it to store Docker images. We tag the image using the URL of the repository which is of the format\n{region}-{zone}-docker.pkg.dev/{project-id}/{repository_name}/{artifact_name}\nWe push the image to the same URL and upon the building of the image and its subsequent upload, it will be visible on the Console.\n\n\n2. Create Vertex AI Model resource\nTo deploy our model, we have two steps. First, we must define a Vertex AI Model instance. This is done by importing the container image from Google Artifact Registry.\nIn our case, first, we set devfest-mobilenet-imagenet as the name of our model. We can also provide a small description of the model for documentation purposes.\n\n\n\nimage\n\n\nOnce the model is named, we choose the actual container image from Google Artifact Registry that we are going to use. Circling back to our example, we make use of our previous devfest-mobilenet-demo image as the basis of our model.\n\n\n\nimage\n\n\nIf your solution is more customized (maybe using something other TF-Serving), you can choose to override the CMD or ENTRYPOINT line of your Dockerfile using the Command field during configuration. Options to set new Environment Variables or Arguments to the process are also present.\n\n\n\nimage\n\n\nFinally, saving the model will allow you to see it on the Console.\n\n\n\nimage\n\n\n\n\n3. Create Vertex AI Endpoint resource\nWith the Vertex AI Model prepared, we now make use of the console to create a Vertex AI Endpoint resource. This endpoint is what will control the full deployment, be it auto-scaling, security, serving, etc.\nWe can click on ’Create Endpoint`, which will lead us into the endpoint-creation wizard.\nIn the first line, we give a name to our new Endpoint, followed by setting it up in a specific Region. Here, since our project is setup in us-central1 , we are going to be restricted to just that. If we have a multi-region project, we can choose other locations too. We also get the option to set up access to the endpoint. If we choose, we can expose the endpoint freely to the internet, or we can make the endpoint private using a VPC network.\n\n\n\nimage\n\n\nNow, we connect a Vertex AI Model with the Endpoint by using the model we had defined. We get to choose the version of the model we want to deploy. If we have multiple instances serving the model on the endpoint, we can choose to perform a traffic split. But if it is a single instance, we keep it to a full 100% split.\n\n\n\nimage\n\n\nIn this set of options, we can specify the configuration and the number of compute nodes we would like the model to use. These are backed by GCP Compute Engine and are fully managed. This solution, by its design, auto-scales according to demand and constraints. While a minimum number of compute nodes is set to maintain the availability of the service, we can set a maximum number to control costs.\nAn option to set a certain CPU threshold allows us to control when the service will scale up (or down) based on CPU usage.\n\n\n\nimage\n\n\nGPU acceleration is also made available for inference compute, as specified by the Accelerator type. If you have IAM set up within your project and have the permissions policy present in your organization, you can use a Service Account to set what kind of resources the service can requisitions, as an added security check on top of other cost-saving measures.\n\n\n\nimage\n\n\nFinally, we get options to enable and set up model monitoring, which would include the tracking of several metrics like Drift, Skew, etc. which allow us to understand how our model performs in production settings and what kind of changes are necessary to make sure that it performs as expected.\n\n\n\nimage\n\n\nNow, let us see our deployment in action!\n\n\nStep 6: Perform Testing and Inference\nFirst, perform authentication into gcloud SDK on your local machine. Prepare the request in the form of a JSON file that will be sent in the body of the request. Make sure that you include the signature name to use since that becomes an integral part of how the model.\n[suvaditya@fedora ~]$ gcloud auth application-default login\n\n[suvaditya@fedora ~]$ echo {\n            \"instances\": [{\n                \"image_bytes\": {\n                      \"b64\": “&lt;BASE64-OF-IMAGE-HERE&gt;”\n                }\n             }\n            ],\n            \"signature_name\": \"serving_image_b64string\"\n} &gt;&gt; sample_request.json\nFor better readability, add your Vertex AI-provided Endpoint ID and Project ID as environment variables in your shell. Finally, we define a cURL POST request as per the specifications in the code snippet below.\n[suvaditya@fedora ~]$ ENDPOINT_ID=\"&lt;YOUR-ENDPOINT-ID-HERE&gt;\"\nPROJECT_ID=\"YOUR-PROJECT-ID-HERE\"\nINPUT_DATA_FILE=\"sample_request.json\"\n\n[suvaditya@fedora ~]$ curl \\\n-X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json\" \\\nhttps://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/endpoints/${ENDPOINT_ID}:rawPredict \\\n-d \"@${INPUT_DATA_FILE}\"\n\n\nPredictions :\nWhat you see below are the predictions of shape (1, 1000) that represent a tensor with output confidence scores for each of ImageNet’s 1000 classes.\n\n\n\nimage"
  },
  {
    "objectID": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#what-have-we-accomplished",
    "href": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#what-have-we-accomplished",
    "title": "Accelerating Model Deployment using Transfer Learning and Vertex AI",
    "section": "What have we accomplished?",
    "text": "What have we accomplished?\nBy this time, we have completed all our setup. Quite a few things were done within this article. Take a look back at the problems we set out to solve:\n\n\n\nimage\n\n\n\nScale: The solution we made is one that can be used in a production setting. With the capability of Vertex AI and the services for Auto-Scaling and Monitoring, you can leverage state-of-the-art tech to make sure your model is deployed in the most performant way possible.\nCan integrate with CI/CD Pipelines: Our solution is one that can be integrated into CI/CD pipelines due to Google Artifact Registry, which can be as simple as pushing to the registry. You can make use of other GCP Services (or Vertex AI) that will allow you to orchestrate the updates automatically.\nOptimizations: TF-Serving is inherently performant, allowing you to serve your model without any hassle. Due to its low-latency nature and graph-mode execution, performance is not a thing you need to worry about.\nModularity: Each and every one of these components is easily replaceable or can be repaired with relative ease, compared to more rigid or non-debuggable products.\nTracking: With state-of-the-art tracking features, your model performance is only a click away. You can leverage automatic insights from Vertex AI as well."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Suvaditya Mukherjee",
    "section": "",
    "text": "Hello! My name is Suvaditya Mukherjee. I am a student of the Bachelors in Artificial Intelligence Program at NMIMS MPSTME, Mumbai.\nI have played around and dabbled in many different disciplines of Computer Science in a quest to find out what excites me the most, and I have realized that Deep Learning and its applications in solving Real-world problems is the best way I see myself utilizing my skills for bettering the community.\nI am a Google Summer of Code Mentor, Org Admin and a ML Research Engineer Intern at Ivy (lets-unify.ai), former Lead of the Google Developer Student Club at NMIMS MPSTME Mumbai and the former Co-Head of the Technical Software department at the International Society of Automation, NMIMS MPSTME Mumbai. I have formerly interned for 2 months at Mosaic Wellness Pvt. Ltd. as a Software Engineer Intern as well.\n\n\n\nDeep Learning : Tensorflow/Keras, PyTorch, Weights&Biases, Optuna, JAX\nMachine Learning : Scikit-Learn, Pandas, Matplotlib, NumPy, CuPy, Seaborn, OpenCV\nCloud Technologies : Amazon Web Services, Google Cloud, Heroku (Salesforce)\nMobile App Development : Java, Flutter(Dart)\nWeb Development : React, Node.js, Bootstrap, HTML/CSS\nLanguages : Python, C++, Java, Dart, JavaScript, Rust (Beginner)\nSpecialized Software : SOLIDWORKS (Beginner), Wireshark (Beginner)\nContainerization : Docker, Kubernetes\n\n\n\n\n\nCoursera: Deep Learning Specialization\nCoursera: Convolutional Neural Networks\nCoursera: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization\nCoursera: Neural Networks and Deep Learning\nCoursera: Structuring Machine Learning Projects\nCoursera: Sequence Models\nGoogle Inc. : GDSC Lead Tenure Completion\nIET MPSTME Hack & Code : Runner-up (2nd Prize) with Aaryadev Chandra, Dev Chandan and Shireen Chand\nUdacity : AWS Machine Learning Foundations\nLinkedIn : Advanced Linux - The Linux Kernel\nLinkedIn : PyTorch Essential Training - Deep Learning\nLinkedIn : Tensorflow - Neural Networks and Working with Tables\nLinkedIn : Unix Essential Training\nCoursera : Building Modern Python Applications on AWS\nKaggle : Intermediate Machine Learning\nKaggle : Intro to Machine Learning\nKaggle : Intro to AI Ethics\nKaggle : Pandas\nKaggle : Python\nStepik : Data Structures\nCoursera : Programming for Everybody(Getting Started with Python)\nGoldman Sachs : Engineering Virtual Program"
  },
  {
    "objectID": "about.html#introduction",
    "href": "about.html#introduction",
    "title": "Suvaditya Mukherjee",
    "section": "",
    "text": "Hello! My name is Suvaditya Mukherjee. I am a student of the Bachelors in Artificial Intelligence Program at NMIMS MPSTME, Mumbai.\nI have played around and dabbled in many different disciplines of Computer Science in a quest to find out what excites me the most, and I have realized that Deep Learning and its applications in solving Real-world problems is the best way I see myself utilizing my skills for bettering the community.\nI am a Google Summer of Code Mentor, Org Admin and a ML Research Engineer Intern at Ivy (lets-unify.ai), former Lead of the Google Developer Student Club at NMIMS MPSTME Mumbai and the former Co-Head of the Technical Software department at the International Society of Automation, NMIMS MPSTME Mumbai. I have formerly interned for 2 months at Mosaic Wellness Pvt. Ltd. as a Software Engineer Intern as well.\n\n\n\nDeep Learning : Tensorflow/Keras, PyTorch, Weights&Biases, Optuna, JAX\nMachine Learning : Scikit-Learn, Pandas, Matplotlib, NumPy, CuPy, Seaborn, OpenCV\nCloud Technologies : Amazon Web Services, Google Cloud, Heroku (Salesforce)\nMobile App Development : Java, Flutter(Dart)\nWeb Development : React, Node.js, Bootstrap, HTML/CSS\nLanguages : Python, C++, Java, Dart, JavaScript, Rust (Beginner)\nSpecialized Software : SOLIDWORKS (Beginner), Wireshark (Beginner)\nContainerization : Docker, Kubernetes\n\n\n\n\n\nCoursera: Deep Learning Specialization\nCoursera: Convolutional Neural Networks\nCoursera: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization\nCoursera: Neural Networks and Deep Learning\nCoursera: Structuring Machine Learning Projects\nCoursera: Sequence Models\nGoogle Inc. : GDSC Lead Tenure Completion\nIET MPSTME Hack & Code : Runner-up (2nd Prize) with Aaryadev Chandra, Dev Chandan and Shireen Chand\nUdacity : AWS Machine Learning Foundations\nLinkedIn : Advanced Linux - The Linux Kernel\nLinkedIn : PyTorch Essential Training - Deep Learning\nLinkedIn : Tensorflow - Neural Networks and Working with Tables\nLinkedIn : Unix Essential Training\nCoursera : Building Modern Python Applications on AWS\nKaggle : Intermediate Machine Learning\nKaggle : Intro to Machine Learning\nKaggle : Intro to AI Ethics\nKaggle : Pandas\nKaggle : Python\nStepik : Data Structures\nCoursera : Programming for Everybody(Getting Started with Python)\nGoldman Sachs : Engineering Virtual Program"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Talks\n\nI deliver talks at local and national meetups in India, around the topics of Machine Learning, MLOps, and Cloud\n\nPresented a talk at DevFest Mumbai 2022: Preparing your ML models for the cloud\nPresented a talk at DevFest Raipur 2022: Accelerating Model deployment using Transfer Learning and Vertex AI\nPresented a talk at DevFest New Delhi 2022: Ivy - Unifying ML With One Line of Code\nPresented a talk at GDSC CRCE 2022: Math in Deep Learning and Technology\nPresented a talk at NMIMS University Mumbai: Hands-on with TensorFlow and Keras\nPresented a talk at Mumbai Women Coders: Improving Digital Experience for Women using Machine Learning\nPresented a talk at The Hackers Meetup: Tackling Security Challenges in ML Systems\n\nIf you’d like to contact me about a talk, feel free to reach out on suvadityamuk@gmail.com"
  },
  {
    "objectID": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#where-to-go-from-here",
    "href": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#where-to-go-from-here",
    "title": "Accelerating Model Deployment using Transfer Learning and Vertex AI",
    "section": "Where to go from here?",
    "text": "Where to go from here?\nMachine Learning can solve a major number of modern-day problems that startups or any kind of problem-solving body faces. Hence, it becomes an important field to stay on top of. But it is equally important to focus on the deployment aspect since, without it, your service is not available to your customers, leading to no value being added.\nBelow are some interesting reference links that can allow you to explore the world of TensorFlow Serving and Vertex AI further.\n\nGuide to TF-Serving (Official TensorFlow documentation)\nPriyanka Vergadia’s video: Accelerate ML Experimentation and Deployment with Vertex AI\nMLOps Architecture using GCP\nWhat is Transfer Learning?"
  },
  {
    "objectID": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#reference-documents",
    "href": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#reference-documents",
    "title": "Accelerating Model Deployment using Transfer Learning and Vertex AI",
    "section": "Reference Documents",
    "text": "Reference Documents\n\nTFX Serving with Docker\nGCP Vertex AI Documentation"
  },
  {
    "objectID": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#acknowledgments",
    "href": "posts/2023-01-03-accelerating-model-deployment-using-transfer-learning-and-vertex-ai.html#acknowledgments",
    "title": "Accelerating Model Deployment using Transfer Learning and Vertex AI",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThank you to Vaibhav Malpani from GDG Cloud Mumbai for providing the GCP credits required to run the demo.\nThank you to Romin Irani for the opportunity to publish the blog in this publication.\nThank you to Sayak Paul for guiding me towards Vertex AI (a very useful demo created by him that I made use of, as a reference point: Deploying ViT on Vertex AI)\nThank you to all the folks who reviewed my slides as part of my talk at GDG DevFest Raipur on the same topic."
  }
]